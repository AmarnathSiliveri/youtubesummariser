import streamlit as st
from deep_translator import GoogleTranslator
from youtube_transcript_api import YouTubeTranscriptApi
from dotenv import load_dotenv
import os
import google.generativeai as genai

from graphviz import Digraph
import streamlit_agraph as agraph
# Session state management
if 'transcript_text' not in st.session_state:
    st.session_state['transcript_text'] = ""
if 'summary' not in st.session_state:
    st.session_state['summary'] = ""
if 'translated_text' not in st.session_state:
    st.session_state['translated_text'] = ""


# Load environment variables
load_dotenv()

# Configure Google API
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Revised prompt
prompt1 = """As a YouTube video summarizer, your task is to analyze the provided transcript text and generate a detailed and well-organized summary of the entire video. Your summary should:
1. Highlight the most important points, key insights, and significant details.
2. Be concise yet comprehensive, ranging between 300-500 words, adjusted for the video's length.
3. Include a final section with a summary in bullet points accompanied by emojis for easy understanding.
4. For code-related videos, include relevant code snippets and explain their significance.

Please produce the summary based on the following transcript:"""

promptM='''As a YouTube video summarizer, your task is to analyze the provided transcript text and generate a detailed and well-organized summary of the entire video. Your summary should:
1. Highlight the most important points, key insights, and significant details.
2. Be concise yet comprehensive,adjusted for the video's length.
produce the suitable outputs based on the following transcript for mindmapping:

'''


# Function to extract transcript details from YouTube video
def extract_transcript_details(youtube_video_url):
    try:
        video_id = youtube_video_url.split("v=")[1].split("&")[0]
        transcript_text = YouTubeTranscriptApi.get_transcript(video_id)
        transcript = " ".join([i["text"] for i in transcript_text])
        return transcript
    except Exception:
        st.warning("Please provide a valid YouTube video URL.")
        return None

# Function to generate summary using Gemini model
def generate_gemini_content(transcript_text, prompt1):
    model = genai.GenerativeModel("gemini-pro")
    response = model.generate_content(prompt1 + transcript_text + " ")
    return response.text

def generate_gemini_content_mindmap(transcript_text, promptM):
    model = genai.GenerativeModel("gemini-pro")
    responseM = model.generate_content(promptM + transcript_text + " ")
    return responseM.text


def translate_text(text, target_language):
    translator = GoogleTranslator(source='auto', target=target_language)
    translated_text = translator.translate(text)
    return translated_text

def generate_mind_map(summary_text):
    dot = Digraph()
    lines = summary_text.split('\n')

    # Set graph attributes for vertical layout and curved edges
    dot.attr(rankdir='TB', splines='ortho', nodesep='1')

    # Set graph attributes for vertical layout and curved edges
    dot.attr(rankdir='TB', splines='ortho', nodesep='1')

    # Create nodes and edges
    for i, line in enumerate(lines):
        line = line.strip()  # Remove leading and trailing spaces
        if line:  # Exclude empty lines
        # Determine node color and style based on main or subtopic
            if i == 0 or line.startswith('-'):
                dot.node(str(i), line, style='bold', color='blue')  # Main topic is bold and blue
            else:
                dot.node(str(i), line, color='green')  # Subtopic is green

            if i > 0:
                parent_line_index = i - 1
                # Check if parent line starts with hyphen to handle multiple levels of subtopics
                while parent_line_index > 0 and lines[parent_line_index].startswith('-'):
                    parent_line_index -= 1
                    dot.edge(str(parent_line_index), str(i), dir='both', arrowhead='none')  # Curvilinear edges with arrowhead none

        # Add semi-transparent watermark using HTML and subgraph
        watermark_graph = dot.subgraph(name='cluster_watermark')
        watermark_graph.attr(style='invisible')  # Make subgraph invisible
        watermark_graph.node('watermark', label=f"""
        <TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" WIDTH="100%">
            <TR><TD COLSPAN="2" BGCOLOR="white" ALIGN="CENTER" VALIGN="MIDDLE">
            <FONT COLOR="gray" POINT-SIZE="30">{watermark_text}</FONT>
            </TD></TR>
            <TR><TD COLSPAN="2" BGCOLOR="transparent" HEIGHT="100%" ALIGN="CENTER" VALIGN="MIDDLE"></TD></TR>
        </TABLE>
        """, shape='plaintext')
        watermark_graph.attr('label', watermark_text)  # Set watermark text as subgraph label (optional)

        # Connect watermark subgraph to the main graph at the top-left corner (adjust as needed)
        dot.edge('watermark', lines[0])

    return dot
# Streamlit app
st.title("YouTube Transcript to Detailed Notes Converter")

# Session state management
if 'transcript_text' not in st.session_state:
    st.session_state['transcript_text'] = ""
if 'summary' not in st.session_state:
    st.session_state['summary'] = ""
if 'translated_text' not in st.session_state:
    st.session_state['translated_text'] = ""
if 'mindmaptext' not in st.session_state:
    st.session_state['mindmaptext']=""
# Input fields
youtube_link = st.text_input("Enter YouTube Video Link:")

# Display thumbnail
if youtube_link:
    video_link = youtube_link.split("v=")[1].split("&")[0]
    st.image(f"http://img.youtube.com/vi/{video_link}/0.jpg", use_column_width=True)

# Generate detailed notes
if st.button("Get Detailed Notes"):
    transcript_text = extract_transcript_details(youtube_link)
    if transcript_text:
        summary = generate_gemini_content(transcript_text, prompt1)
        st.session_state['transcript_text'] = transcript_text
        st.session_state['summary'] = summary
    


# Display the summary with custom font
if st.session_state['summary']:
    st.markdown("## Detailed Notes:")
    st.markdown(
        f"""
        <div style="font-family: 'Arial', sans-serif; font-size: 16px; color: #ffff;">
            {st.session_state['summary']}
        </div>
        """,
        unsafe_allow_html=True
    )
    with st.expander("Translate"):
        language_names = {
            "en": "English", "hi": "Hindi", "bn": "Bengali", "gu": "Gujarati", "kn": "Kannada",
            "ml": "Malayalam", "mr": "Marathi", "pa": "Punjabi", "ta": "Tamil", "te": "Telugu",
            "as": "Assamese", "or": "Odia", "ur": "Urdu", "ne": "Nepali", "es": "Spanish", "fr": "French",
            "de": "German", "it": "Italian", "pt": "Portuguese", "ru": "Russian", "zh": "Chinese", 
            "ja": "Japanese", "ko": "Korean", "ar": "Arabic", "tr": "Turkish", "vi": "Vietnamese",
            "th": "Thai", "ms": "Malay", "id": "Indonesian", "fa": "Persian"
        }
        target_language = st.selectbox("Select target language:", list(language_names.values()))

        if st.button("Translate Summary"):
            target_language_code = list(language_names.keys())[list(language_names.values()).index(target_language)]
            translated_text = translate_text(st.session_state['summary'], target_language_code)
            st.session_state['translated_text'] = translated_text

        st.text_area("Translated Text", value=st.session_state['translated_text'], height=200)

 
if st.button("Mind Map Generator"):
    transcript_text = extract_transcript_details(youtube_link)
    if transcript_text:
        mindmap_summary = generate_gemini_content_mindmap(transcript_text,promptM)
        st.session_state['mindmaptext']=mindmap_summary
        # Mind Map Section
        st.markdown("## Mind Map of the Summary")
        mind_map =generate_mind_map(st.session_state['mindmaptext'])
        st.graphviz_chart(mind_map.source)